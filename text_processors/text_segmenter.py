#!/usr/bin/env python3
"""
Text segmenter for thematic analysis of transcribed text.
Uses LLM to split text into meaningful thematic blocks.
"""

import os
import json
import time
import argparse
import requests
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
from dotenv import load_dotenv


class TextSegmenter:
    def __init__(self, config_file: Optional[str] = None):
        """
        Initialize text segmenter
        
        Args:
            config_file: Path to configuration file
        """
        self.config_file = config_file
        self.load_config()
        
        # Statistics
        self.stats = {
            'api_calls': 0,
            'total_tokens_used': 0,
            'segments_created': 0
        }
    
    def load_config(self):
        """Load configuration from environment or config file"""
        try:
            # –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–π config.env (–µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
            base_config = Path('config.env')
            if base_config.exists():
                load_dotenv(base_config, override=False)
            
            # –ó–∞—Ç–µ–º –∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∑–∞–¥–∞–Ω–∏—è (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω), –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—è –∑–Ω–∞—á–µ–Ω–∏—è
            if self.config_file:
                config_path = Path(self.config_file)
                if config_path.exists():
                    load_dotenv(config_path, override=True)
            
            # –ï—Å–ª–∏ –∫–æ–Ω—Ñ–∏–≥ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å .env –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if not self.config_file:
                load_dotenv(override=False)
        except ImportError:
            pass
        
        # API configuration
        self.api_key = os.getenv('OPENROUTER_API_KEY')
        self.base_url = os.getenv('OPENROUTER_BASE_URL', 'https://openrouter.ai/api/v1')
        
        # Model configuration
        self.model = os.getenv('DEFAULT_MODEL', 'anthropic/claude-3.5-sonnet')
        self.budget_model = os.getenv('BUDGET_MODEL', 'meta-llama/llama-3.1-8b-instruct')
        self.quality_model = os.getenv('QUALITY_MODEL', 'openai/gpt-4o')
        
        # Processing parameters
        self.temperature = float(os.getenv('DEFAULT_TEMPERATURE', '0.3'))
        self.max_tokens = int(os.getenv('DEFAULT_MAX_TOKENS', '4000'))
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç —ç—Ç–æ—Ç –ª–∏–º–∏—Ç, –æ–Ω –±—É–¥–µ—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ —á–∞—Å—Ç–∏
        self.max_input_tokens = int(os.getenv('MAX_TOKENS', '30000'))
        
        # Headers for API requests
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/your-repo/bookreader",
            "X-Title": "Text Segmenter"
        }
    
    def _call_llm(self, prompt: str, system: Optional[str] = None, 
                  model: Optional[str] = None, retry_count: int = 3,
                  max_tokens: Optional[int] = None) -> Optional[str]:
        """
        Call LLM API with retry logic
        
        Args:
            prompt: User prompt
            system: System prompt (optional)
            model: Model to use (optional)
            retry_count: Number of retry attempts
            max_tokens: Maximum tokens for response (optional, defaults to self.max_tokens)
            
        Returns:
            LLM response or None
        """
        payload = {
            "model": model or self.model,
            "messages": [],
            "temperature": self.temperature,
            "max_tokens": max_tokens or self.max_tokens
        }
        
        if system:
            payload["messages"].append({"role": "system", "content": system})
        payload["messages"].append({"role": "user", "content": prompt})
        
        for attempt in range(retry_count):
            try:
                self.stats["api_calls"] += 1
                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=120
                )
                
                if response.status_code == 200:
                    data = response.json()
                    content = data["choices"][0]["message"]["content"].strip()
                    
                    # Update token usage statistics
                    if "usage" in data:
                        self.stats["total_tokens_used"] += data["usage"].get("total_tokens", 0)
                    
                    return content
                elif response.status_code == 429:
                    wait_time = 2 ** (attempt + 1)
                    print(f"‚è≥ Rate limit, –æ–∂–∏–¥–∞–Ω–∏–µ {wait_time} —Å–µ–∫—É–Ω–¥...")
                    time.sleep(wait_time)
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ API (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {response.status_code}")
                    if attempt < retry_count - 1:
                        time.sleep(2 ** attempt)
                        
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(2 ** attempt)
        
        return None
    
    def estimate_tokens(self, text: str) -> int:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        –ü—Ä–∏–º–µ—Ä–Ω–æ 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        # –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        return len(text) // 4
    
    def load_transcript_json(self, transcript_json_path: str) -> Optional[Dict[str, Any]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç transcript.json –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–∞–π–º—Å—Ç–µ–º–ø–æ–≤
        
        Args:
            transcript_json_path: –ü—É—Ç—å –∫ transcript.json
            
        Returns:
            –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏–ª–∏ None
        """
        try:
            path = Path(transcript_json_path)
            if not path.exists():
                return None
            
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã —Å —Ç–∞–π–º—Å—Ç–µ–º–ø–∞–º–∏
            if 'segments' in data and len(data['segments']) > 0:
                if 'start' in data['segments'][0] and 'end' in data['segments'][0]:
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω transcript.json —Å —Ç–∞–π–º—Å—Ç–µ–º–ø–∞–º–∏: {len(data['segments'])} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
                    return data
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å transcript.json: {e}")
            return None
    
    def find_text_position_in_transcript(self, search_text: str, transcript_data: Dict[str, Any]) -> Optional[tuple]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –ø–æ–∑–∏—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç start –∏ end –≤—Ä–µ–º–µ–Ω–∞
        
        Args:
            search_text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å —á–∞—Å—Ç—å—é)
            transcript_data: –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏–∑ transcript.json
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (start_time, end_time) –≤ —Å–µ–∫—É–Ω–¥–∞—Ö –∏–ª–∏ None
        """
        if not transcript_data or 'segments' not in transcript_data:
            return None
        
        segments = transcript_data['segments']
        full_text = transcript_data.get('text', '')
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ (—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã)
        search_text_normalized = ' '.join(search_text.split())
        full_text_normalized = ' '.join(full_text.split())
        
        # –ò—â–µ–º –Ω–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞
        start_pos = full_text_normalized.find(search_text_normalized)
        if start_pos == -1:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –ø–µ—Ä–≤—ã–º —Å–ª–æ–≤–∞–º
            first_words = ' '.join(search_text_normalized.split()[:10])
            start_pos = full_text_normalized.find(first_words)
            if start_pos == -1:
                return None
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
        text_ratio_start = start_pos / len(full_text_normalized) if len(full_text_normalized) > 0 else 0
        text_ratio_end = (start_pos + len(search_text_normalized)) / len(full_text_normalized) if len(full_text_normalized) > 0 else 0
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–µ–µ –≤—Ä–µ–º—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
        if not segments:
            return None
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤–æ–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è
        first_start = segments[0].get('start', 0)
        last_end = segments[-1].get('end', 0)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
        total_duration = last_end - first_start
        
        start_time = first_start + (text_ratio_start * total_duration)
        end_time = first_start + (text_ratio_end * total_duration)
        
        return (start_time, end_time)
    
    def format_time(self, seconds: float) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –≤ —Ñ–æ—Ä–º–∞—Ç HH:MM:SS
        
        Args:
            seconds: –í—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            –°—Ç—Ä–æ–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ HH:MM:SS
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"
    
    def assign_timestamps_to_segments(self, segments: List[Dict[str, Any]], 
                                      transcript_data: Optional[Dict[str, Any]],
                                      full_text: str) -> List[Dict[str, Any]]:
        """
        –ù–∞–∑–Ω–∞—á–∞–µ—Ç —Ç–∞–π–º—Å—Ç–µ–º–ø—ã —Å–µ–≥–º–µ–Ω—Ç–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ transcript.json
        
        Args:
            segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º
            transcript_data: –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –∏–∑ transcript.json
            full_text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ start_time –∏ end_time
        """
        if not transcript_data:
            return segments
        
        print("üïê –í—ã—á–∏—Å–ª—è–µ–º —Ç–∞–π–º—Å—Ç–µ–º–ø—ã –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        full_text_normalized = ' '.join(full_text.split())
        current_pos = 0
        
        for segment in segments:
            content = segment.get('content', '')
            if not content:
                continue
            
            # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –ø–æ–ª–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
            content_normalized = ' '.join(content.split())
            
            # –ò—â–µ–º –æ—Ç —Ç–µ–∫—É—â–µ–π –ø–æ–∑–∏—Ü–∏–∏
            search_start = full_text_normalized.find(content_normalized, current_pos)
            
            if search_start == -1:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –ø–µ—Ä–≤—ã–º —Å–ª–æ–≤–∞–º
                first_words = ' '.join(content_normalized.split()[:5])
                search_start = full_text_normalized.find(first_words, current_pos)
            
            if search_start == -1:
                # –ù–µ –Ω–∞—à–ª–∏ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
            text_ratio_start = search_start / len(full_text_normalized) if len(full_text_normalized) > 0 else 0
            text_ratio_end = (search_start + len(content_normalized)) / len(full_text_normalized) if len(full_text_normalized) > 0 else 0
            
            # –ù–∞—Ö–æ–¥–∏–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –∏–∑ transcript
            transcript_segments = transcript_data.get('segments', [])
            if not transcript_segments:
                continue
            
            first_start = transcript_segments[0].get('start', 0)
            last_end = transcript_segments[-1].get('end', 0)
            total_duration = last_end - first_start
            
            # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è
            start_time_seconds = first_start + (text_ratio_start * total_duration)
            end_time_seconds = first_start + (text_ratio_end * total_duration)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º—è
            segment['start_time'] = self.format_time(start_time_seconds)
            segment['end_time'] = self.format_time(end_time_seconds)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–æ–∏—Å–∫–∞
            current_pos = search_start + len(content_normalized)
        
        return segments
    
    def split_text_into_chunks(self, text: str, estimated_prompt_tokens: int = 500) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω –ø—Ä–µ–≤—ã—à–∞–µ—Ç max_input_tokens
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
            estimated_prompt_tokens: –û—Ü–µ–Ω–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ (–±–µ–∑ —Ç–µ–∫—Å—Ç–∞)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞
        """
        text_tokens = self.estimate_tokens(text)
        # –£—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ
        available_tokens = self.max_input_tokens - estimated_prompt_tokens
        
        if text_tokens <= available_tokens:
            return [text]
        
        print(f"üìä –¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({text_tokens} —Ç–æ–∫–µ–Ω–æ–≤ > {available_tokens} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö)")
        print(f"   –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ ~{available_tokens} —Ç–æ–∫–µ–Ω–æ–≤...")
        
        chunks = []
        # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        chunk_size_chars = available_tokens * 4  # ~4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ —Å –Ω–µ–±–æ–ª—å—à–∏–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        overlap_chars = chunk_size_chars // 10  # 10% –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
        
        # –¢–æ—á–∫–∏ —Ä–∞–∑—Ä—ã–≤–∞ —Ç–µ–∫—Å—Ç–∞ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
        break_points = ['\n\n', '. ', '; ', '! ', '? ', ' ']
        
        start = 0
        while start < len(text):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–π —á–∞—Å—Ç–∏
            ideal_end = start + chunk_size_chars
            end = min(ideal_end, len(text))
            
            # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å, –±–µ—Ä–µ–º –≤–µ—Å—å –æ—Å—Ç–∞—Ç–æ–∫
            if end >= len(text):
                chunk = text[start:].strip()
                if chunk:
                    chunks.append(chunk)
                break
            
            # –ò—â–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞ (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
            chunk_text = text[start:ideal_end + overlap_chars]
            best_break = ideal_end - start
            
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞
            for break_point in break_points:
                # –ò—â–µ–º —Ä–∞–∑—Ä—ã–≤ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 20% —á–∞—Å—Ç–∏
                search_start = int(len(chunk_text) * 0.8)
                break_pos = chunk_text.rfind(break_point, search_start)
                if break_pos > 0:
                    best_break = break_pos + len(break_point)
                    break
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
            chunk = text[start:start + best_break].strip()
            if chunk:
                chunks.append(chunk)
            
            # –î–ª—è —Å–ª–µ–¥—É—é—â–µ–π —á–∞—Å—Ç–∏ –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω–µ–±–æ–ª—å—à–æ–≥–æ –æ—Ç–∫–∞—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            if start + best_break < len(text):
                # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º—Å—è –Ω–µ–º–Ω–æ–≥–æ –Ω–∞–∑–∞–¥ –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                overlap_start = max(start, start + best_break - overlap_chars)
                # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                for break_point in break_points:
                    overlap_break = text[overlap_start:start + best_break].find(break_point)
                    if overlap_break > 0:
                        overlap_start += overlap_break + len(break_point)
                        break
                start = overlap_start
            else:
                start = start + best_break
        
        print(f"   ‚úÖ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
        return chunks
    
    def merge_segments(self, all_segments: List[List[Dict[str, Any]]], original_text_length: int) -> List[Dict[str, Any]]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞
        
        Args:
            all_segments: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å)
            original_text_length: –î–ª–∏–Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        """
        if not all_segments:
            return []
        
        if len(all_segments) == 1:
            return all_segments[0]
        
        merged = []
        current_index = 1
        
        for part_segments in all_segments:
            for segment in part_segments:
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å —Å–µ–≥–º–µ–Ω—Ç–∞
                segment_copy = segment.copy()
                segment_copy['index'] = current_index
                merged.append(segment_copy)
                current_index += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ
        total_content_length = sum(len(seg.get('content', '')) for seg in merged)
        coverage = (total_content_length / original_text_length * 100) if original_text_length > 0 else 0
        
        print(f"üìä –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(merged)}")
        print(f"üìä –ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞: {coverage:.1f}%")
        
        if coverage < 80:
            print(f"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ –º–µ–Ω–µ–µ 80% ({coverage:.1f}%)")
        
        return merged
    
    def create_segmentation_prompt(self, text: str, segments_count: int) -> str:
        """
        Create prompt for text segmentation
        
        Args:
            text: Text to segment
            segments_count: Desired number of segments
            
        Returns:
            Formatted prompt
        """
        text_length = len(text)
        estimated_chars_per_segment = text_length // segments_count
        
        return f"""–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ–∫—Å—Ç–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–ó–ê–î–ê–ß–ê: –†–∞–∑–¥–µ–ª–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ {segments_count} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤.

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- –ë–ª–æ–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–Ω–æ–π –¥–ª–∏–Ω—ã (~{estimated_chars_per_segment} —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –±–ª–æ–∫)
- –°–æ—Ö—Ä–∞–Ω—è–π —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫, –µ—Å–ª–∏ —ç—Ç–æ –≤–∞–∂–Ω–æ
- –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –∫—Ä–∞—Ç–∫–æ–µ, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
- –ù–µ —Ä–∞–∑—Ä—ã–≤–∞–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏
- –ë–ª–æ–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
- –í–ê–ñ–ù–û: –û–±—Ä–∞–±–æ—Ç–∞–π –í–ï–°–¨ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞! –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π—Å—è –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω–µ.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê ‚Äî —Å—Ç—Ä–æ–≥–æ JSON:
{{
  "segments": [
    {{
      "index": 1,
      "title": "–ù–∞–∑–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞",
      "content": "–¢–µ–∫—Å—Ç –±–ª–æ–∫–∞...",
      "start_time": "00:00:00",
      "end_time": "00:05:30"
    }},
    {{
      "index": 2,
      "title": "–ù–∞–∑–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞",
      "content": "–¢–µ–∫—Å—Ç –±–ª–æ–∫–∞...",
      "start_time": "00:05:30",
      "end_time": "00:10:15"
    }}
  ]
}}

–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: 
- –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π
- –ü–æ–ª—è start_time –∏ end_time –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–º–∏ (–æ–Ω–∏ –±—É–¥—É—Ç –≤—ã—á–∏—Å–ª–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
- –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –æ–±—Ä–∞–±–æ—Ç–∞–π –í–ï–°–¨ —Ç–µ–∫—Å—Ç –æ—Ç –Ω–∞—á–∞–ª–∞ –¥–æ –∫–æ–Ω—Ü–∞ - –≤—Å–µ {segments_count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–æ–ª–∂–Ω—ã –ø–æ–∫—Ä—ã–≤–∞—Ç—å –≤–µ—Å—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤
- –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∑–∞–∫–∞–Ω—á–∏–≤–∞—Ç—å—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞

–¢–ï–ö–°–¢ –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê (–≤—Å–µ–≥–æ {text_length} —Å–∏–º–≤–æ–ª–æ–≤):
{text}

JSON –û–¢–í–ï–¢ (–¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ä–æ–≤–Ω–æ {segments_count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤, –ø–æ–∫—Ä—ã–≤–∞—é—â–∏—Ö –≤–µ—Å—å —Ç–µ–∫—Å—Ç):"""
    
    def check_text_coverage(self, segments: List[Dict[str, Any]], full_text: str, 
                           min_coverage: float = 0.90) -> Tuple[bool, float, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
        
        Args:
            segments: –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            full_text: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
            min_coverage: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (0.0-1.0)
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–ø–æ–∫—Ä—ã—Ç–∏–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ, –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è, —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞)
        """
        if not segments:
            return False, 0.0, full_text
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        segments_content = ' '.join([seg.get('content', '') for seg in segments])
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã)
        segments_normalized = ' '.join(segments_content.split())
        full_normalized = ' '.join(full_text.split())
        
        # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        last_segment_content = segments[-1].get('content', '')
        if not last_segment_content:
            return False, 0.0, full_text
        
        last_segment_normalized = ' '.join(last_segment_content.split())
        last_pos = full_normalized.find(last_segment_normalized)
        
        if last_pos >= 0:
            text_after = full_normalized[last_pos + len(last_segment_normalized):].strip()
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ –ø–µ—Ä–≤—ã–º —Å–ª–æ–≤–∞–º
            first_words = ' '.join(last_segment_normalized.split()[:10])
            last_pos = full_normalized.find(first_words)
            if last_pos >= 0:
                text_after = full_normalized[last_pos + len(first_words):].strip()
            else:
                text_after = full_normalized
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ
        covered_length = len(full_normalized) - len(text_after)
        coverage = covered_length / len(full_normalized) if len(full_normalized) > 0 else 0.0
        
        return coverage >= min_coverage, coverage, text_after
    
    def segment_text_chunk(self, text: str, segments_count: int, 
                          model_choice: str = "default", chunk_info: str = "") -> Optional[List[Dict[str, Any]]]:
        """
        –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –æ–¥–Ω—É —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            segments_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤ —ç—Ç–æ–π —á–∞—Å—Ç–∏
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            chunk_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–∞—Å—Ç–∏ (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not text or not text.strip():
            return None
        
        # Select model
        model = self.model
        if model_choice == "budget":
            model = self.budget_model
        elif model_choice == "quality":
            model = self.quality_model
        
        # Create prompt
        prompt = self.create_segmentation_prompt(text, segments_count)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        # –ü—Ä–∏–º–µ—Ä–Ω–æ: —Ç–µ–∫—Å—Ç / 4 (—Ç–æ–∫–µ–Ω—ã –Ω–∞ —Å–∏–º–≤–æ–ª) + –ø—Ä–æ–º–ø—Ç + –∑–∞–ø–∞—Å –¥–ª—è JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        # –î–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤: –ø—Ä–∏–º–µ—Ä–Ω–æ 100-200 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç (–∑–∞–≥–æ–ª–æ–≤–æ–∫ + –∫–æ–Ω—Ç–µ–Ω—Ç + JSON)
        estimated_response_tokens = segments_count * 200 + 500  # –ó–∞–ø–∞—Å –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã JSON
        
        # Call LLM —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º max_tokens
        response = self._call_llm(prompt, model=model, max_tokens=max(self.max_tokens, estimated_response_tokens))
        if not response:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM {chunk_info}")
            return None
        
        # Parse JSON response
        try:
            # Clean response (remove code fences if present)
            if response.startswith("```") and response.endswith("```"):
                lines = response.splitlines()
                if len(lines) >= 2:
                    response = "\n".join(lines[1:-1])
            
            data = json.loads(response)
            segments = data.get("segments", [])
            
            if not segments:
                print(f"‚ùå LLM –Ω–µ —Å–æ–∑–¥–∞–ª —Å–µ–≥–º–µ–Ω—Ç—ã {chunk_info}")
                return None
            
            # Validate segments
            for i, segment in enumerate(segments):
                if not isinstance(segment, dict):
                    print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–∞ {i} {chunk_info}")
                    return None
                
                required_fields = ["index", "title", "content"]
                for field in required_fields:
                    if field not in segment:
                        print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ '{field}' –≤ —Å–µ–≥–º–µ–Ω—Ç–µ {i} {chunk_info}")
                        return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞
            is_covered, coverage, text_after = self.check_text_coverage(segments, text, min_coverage=0.85)
            
            if not is_covered:
                remaining_chars = len(text_after)
                print(f"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ {chunk_info}: –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {coverage*100:.1f}% (–æ–∂–∏–¥–∞–ª–æ—Å—å ‚â•85%)")
                print(f"   –ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {remaining_chars} —Å–∏–º–≤–æ–ª–æ–≤")
                if remaining_chars > 100:
                    print(f"   –ù–∞—á–∞–ª–æ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {text_after[:200]}...")
                    print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}, –æ–∂–∏–¥–∞–ª–æ—Å—å: {segments_count}")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –¥–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–π —Å–µ–≥–º–µ–Ω—Ç
                    if remaining_chars > 50 and len(segments) < segments_count * 2:  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
                        print(f"   –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç –¥–ª—è –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è —Ç–µ–∫—Å—Ç–∞...")
                        # –°–æ–∑–¥–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç –∏–∑ –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è —Ç–µ–∫—Å—Ç–∞
                        additional_prompt = self.create_segmentation_prompt(text_after, 1)
                        additional_response = self._call_llm(additional_prompt, model=model, max_tokens=2000)
                        
                        if additional_response:
                            try:
                                # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
                                if additional_response.startswith("```") and additional_response.endswith("```"):
                                    lines = additional_response.splitlines()
                                    if len(lines) >= 2:
                                        additional_response = "\n".join(lines[1:-1])
                                
                                additional_data = json.loads(additional_response)
                                additional_segments = additional_data.get("segments", [])
                                
                                if additional_segments:
                                    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                                    for additional_seg in additional_segments:
                                        additional_seg['index'] = len(segments) + 1
                                    segments.extend(additional_segments)
                                    print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç")
                                    
                                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –µ—â–µ —Ä–∞–∑
                                    is_covered, coverage, _ = self.check_text_coverage(segments, text, min_coverage=0.85)
                                    if is_covered:
                                        print(f"   ‚úÖ –ü–æ–∫—Ä—ã—Ç–∏–µ —É–ª—É—á—à–µ–Ω–æ –¥–æ {coverage*100:.1f}%")
                                    else:
                                        print(f"   ‚ö†Ô∏è –ü–æ–∫—Ä—ã—Ç–∏–µ –≤—Å–µ –µ—â–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ: {coverage*100:.1f}%")
                            except Exception as e:
                                print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç: {e}")
            
            return segments
            
        except json.JSONDecodeError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON {chunk_info}: {e}")
            print(f"–û—Ç–≤–µ—Ç LLM: {response[:200]}...")
            return None
        except Exception as e:
            print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ {chunk_info}: {e}")
            return None
    
    def segment_text(self, text: str, segments_count: int, 
                    model_choice: str = "default",
                    transcript_data: Optional[Dict[str, Any]] = None) -> Optional[List[Dict[str, Any]]]:
        """
        Segment text into thematic blocks
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω –ø—Ä–µ–≤—ã—à–∞–µ—Ç max_input_tokens
        
        Args:
            text: Text to segment
            segments_count: Number of segments to create
            model_choice: Model to use (default/budget/quality)
            transcript_data: –î–∞–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–∞–π–º—Å—Ç–µ–º–ø–æ–≤
            
        Returns:
            List of segments or None if failed
        """
        if not text or not text.strip():
            print("‚ùå –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
            return None
        
        # Select model
        model = self.model
        if model_choice == "budget":
            model = self.budget_model
        elif model_choice == "quality":
            model = self.quality_model
        
        print(f"üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å: {model}")
        print(f"üìä –°–æ–∑–¥–∞–µ–º {segments_count} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤...")
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç
        chunks = self.split_text_into_chunks(text)
        
        if len(chunks) == 1:
            # –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ —Ä–∞–Ω—å—à–µ
            segments = self.segment_text_chunk(chunks[0], segments_count, model_choice)
            if segments:
                # –ù–∞–∑–Ω–∞—á–∞–µ–º —Ç–∞–π–º—Å—Ç–µ–º–ø—ã
                segments = self.assign_timestamps_to_segments(segments, transcript_data, text)
                self.stats["segments_created"] = len(segments)
                print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(segments)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤")
            return segments
        else:
            # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º
            print(f"üì¶ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ —á–∞—Å—Ç—è–º ({len(chunks)} —á–∞—Å—Ç–µ–π)...")
            
            all_segments = []
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏
            segments_per_chunk = max(1, segments_count // len(chunks))
            remaining_segments = segments_count - (segments_per_chunk * len(chunks))
            
            for i, chunk in enumerate(chunks):
                chunk_segments_count = segments_per_chunk
                if i < remaining_segments:
                    chunk_segments_count += 1
                
                chunk_info = f"(—á–∞—Å—Ç—å {i+1}/{len(chunks)})"
                print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç—å {i+1}/{len(chunks)}: ~{chunk_segments_count} —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")
                
                chunk_segments = self.segment_text_chunk(chunk, chunk_segments_count, model_choice, chunk_info)
                
                if chunk_segments:
                    all_segments.append(chunk_segments)
                    print(f"‚úÖ –ß–∞—Å—Ç—å {i+1} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: —Å–æ–∑–¥–∞–Ω–æ {len(chunk_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
                else:
                    print(f"‚ö†Ô∏è –ß–∞—Å—Ç—å {i+1} –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if i < len(chunks) - 1:
                    time.sleep(1)
            
            if not all_segments:
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–Ω—É —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞")
                return None
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π
            merged_segments = self.merge_segments(all_segments, len(text))
            
            # –ù–∞–∑–Ω–∞—á–∞–µ–º —Ç–∞–π–º—Å—Ç–µ–º–ø—ã –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
            merged_segments = self.assign_timestamps_to_segments(merged_segments, transcript_data, text)
            
            self.stats["segments_created"] = len(merged_segments)
            print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(merged_segments)} —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –±–ª–æ–∫–æ–≤")
            
            return merged_segments
    
    def save_segments(self, segments: List[Dict[str, Any]], output_file: str) -> bool:
        """
        Save segments to JSON file
        
        Args:
            segments: List of segments
            output_file: Output file path
            
        Returns:
            True if successful
        """
        try:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Create output data
            output_data = {
                "metadata": {
                    "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "segments_count": len(segments),
                    "model_used": self.model,
                    "api_calls": self.stats["api_calls"],
                    "tokens_used": self.stats["total_tokens_used"]
                },
                "segments": segments
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ –°–µ–≥–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {e}")
            return False
    
    def process_text_file(self, input_file: str, output_file: str, 
                         segments_count: int, model_choice: str = "default",
                         transcript_json: Optional[str] = None) -> bool:
        """
        Process text file and create segments
        
        Args:
            input_file: Input text file
            output_file: Output JSON file
            segments_count: Number of segments
            model_choice: Model choice
            transcript_json: Optional path to transcript.json for timestamps
            
        Returns:
            True if successful
        """
        try:
            # Read input file
            input_path = Path(input_file)
            if not input_path.exists():
                print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_path}")
                return False
            
            text = input_path.read_text(encoding='utf-8')
            if not text.strip():
                print("‚ùå –§–∞–π–ª –ø—É—Å—Ç")
                return False
            
            print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç: {len(text):,} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º transcript.json –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω —è–≤–Ω–æ, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            transcript_data = None
            if transcript_json:
                transcript_data = self.load_transcript_json(transcript_json)
            else:
                # –ò—â–µ–º transcript.json –≤ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —á—Ç–æ –∏ input_file
                input_dir = input_path.parent
                transcript_path = input_dir / "transcript.json"
                if transcript_path.exists():
                    transcript_data = self.load_transcript_json(str(transcript_path))
            
            # –ï—Å–ª–∏ transcript.json –∑–∞–≥—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –Ω–µ–≥–æ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∏–∑ input_file
            text_for_segmentation = text
            if transcript_data and 'text' in transcript_data:
                text_for_segmentation = transcript_data['text']
                print("üìÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∏–∑ transcript.json –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏")
            
            # Segment text
            segments = self.segment_text(text_for_segmentation, segments_count, model_choice, transcript_data)
            if not segments:
                return False
            
            # Save segments
            success = self.save_segments(segments, output_file)
            if success:
                print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
                print(f"   - –°–æ–∑–¥–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
                print(f"   - API –≤—ã–∑–æ–≤–æ–≤: {self.stats['api_calls']}")
                print(f"   - –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {self.stats['total_tokens_used']}")
            
            return success
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(description="–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏")
    parser.add_argument('input_file', help='–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª')
    parser.add_argument('--output', '-o', required=True, help='–í—ã—Ö–æ–¥–Ω–æ–π JSON —Ñ–∞–π–ª')
    parser.add_argument('--segments', '-s', type=int, default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤')
    parser.add_argument('--config', help='–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--model', choices=['default', 'budget', 'quality'], 
                       default='default', help='–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏')
    parser.add_argument('--transcript-json', help='–ü—É—Ç—å –∫ transcript.json –¥–ª—è —Ç–∞–π–º—Å—Ç–µ–º–ø–æ–≤')
    
    args = parser.parse_args()
    
    segmenter = TextSegmenter(args.config)
    
    success = segmenter.process_text_file(
        args.input_file,
        args.output,
        args.segments,
        args.model,
        args.transcript_json
    )
    
    if success:
        print("‚úÖ –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
        return 0
    else:
        print("‚ùå –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å")
        return 1


if __name__ == "__main__":
    exit(main())
