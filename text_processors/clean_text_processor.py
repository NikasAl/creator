#!/usr/bin/env python3
"""
–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
–§–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
"""

import os
import sys
import json
import time
import argparse
import requests
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dotenv import load_dotenv
import re
from datetime import datetime


class CleanTextProcessor:
    def __init__(self, config_file: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Args:
            config_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ .env
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.load_config(config_file)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if not self.api_key:
            raise ValueError("API –∫–ª—é—á OpenRouter –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API
        self.base_url = "https://openrouter.ai/api/v1"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/your-repo/clean-text-processor",
            "X-Title": "Clean Text Processor"
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_chunks': 0,
            'processed_chunks': 0,
            'failed_chunks': 0,
            'total_characters': 0,
            'total_tokens_used': 0,
            'processing_time': 0,
            'api_calls': 0,
            'filtered_elements': 0
        }
    
    def load_config(self, config_file: str = None):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ .env —Ñ–∞–π–ª–∞"""
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        if config_file and Path(config_file).exists():
            load_dotenv(config_file)
        else:
            # –ò—â–µ–º .env —Ñ–∞–π–ª –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            env_files = ['.env', 'config.env', 'settings.env']
            for env_file in env_files:
                if Path(env_file).exists():
                    load_dotenv(env_file)
                    break
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.api_key = os.getenv('OPENROUTER_API_KEY')
        self.model = os.getenv('DEFAULT_MODEL', 'anthropic/claude-3.5-sonnet')
        self.chunk_size = int(os.getenv('DEFAULT_CHUNK_SIZE', '3000'))
        self.temperature = float(os.getenv('DEFAULT_TEMPERATURE', '0.1'))
        self.max_tokens = int(os.getenv('DEFAULT_MAX_TOKENS', '4000'))
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.budget_model = os.getenv('BUDGET_MODEL', 'meta-llama/llama-3.1-8b-instruct')
        self.quality_model = os.getenv('QUALITY_MODEL', 'openai/gpt-4o')
        
        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞:")
        print(f"   –ú–æ–¥–µ–ª—å: {self.model}")
        print(f"   –†–∞–∑–º–µ—Ä —á–∞—Å—Ç–∏: {self.chunk_size}")
        print(f"   –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {self.temperature}")
    
    def detect_book_info(self, text: str) -> Dict[str, str]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–µ
        """
        book_info = {
            'title': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–Ω–∏–≥–∞',
            'author': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä',
            'topic': '–û–±—â–∞—è —Ç–µ–º–∞'
        }
        
        # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ (–æ–±—ã—á–Ω–æ –≤ –Ω–∞—á–∞–ª–µ)
        lines = text.split('\n')[:20]  # –ü–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
        title_patterns = [
            r'^([–ê-–Ø][–ê-–Ø\s,]+)$',  # –ó–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
            r'^([–ê-–Ø][–∞-—è\s,]+)$',  # –ü–µ—Ä–≤–∞—è –∑–∞–≥–ª–∞–≤–Ω–∞—è
            r'([–ê-–Ø][–ê-–Ø\s]+–Ø–í–õ–ï–ù–ò–Ø[–ê-–Ø\s]+)',  # –®–∏–∑–æ–∏–¥–Ω—ã–µ —è–≤–ª–µ–Ω–∏—è
        ]
        
        for line in lines:
            line = line.strip()
            for pattern in title_patterns:
                match = re.search(pattern, line)
                if match and len(line) > 10:
                    book_info['title'] = line
                    break
            if book_info['title'] != '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–Ω–∏–≥–∞':
                break
        
        # –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞
        author_patterns = [
            r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)\s*[-‚Äì‚Äî]\s*–∞–≤—Ç–æ—Ä',
            r'–ê–≤—Ç–æ—Ä[:\s]+([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)',
            r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)\s*[-‚Äì‚Äî]',
            r'–ì–∞—Ä—Ä–∏\s+–ì–∞–Ω—Ç—Ä–∏–ø',  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è —ç—Ç–æ–π –∫–Ω–∏–≥–∏
        ]
        
        for line in lines:
            for pattern in author_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    book_info['author'] = match.group(1) if match.groups() else match.group(0)
                    break
            if book_info['author'] != '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä':
                break
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        topic_keywords = {
            '–ø—Å–∏—Ö–æ–∞–Ω–∞–ª–∏–∑': '–ü—Å–∏—Ö–æ–∞–Ω–∞–ª–∏–∑ –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è',
            '—à–∏–∑–æ–∏–¥': '–®–∏–∑–æ–∏–¥–Ω—ã–µ —è–≤–ª–µ–Ω–∏—è –≤ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏',
            '–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è': '–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è –∏ –ø—Å–∏—Ö–∏–∞—Ç—Ä–∏—è',
            '—Ç–µ—Ä–∞–ø–∏—è': '–ü—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–∏—è',
            '–ª–∏—á–Ω–æ—Å—Ç—å': '–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è –ª–∏—á–Ω–æ—Å—Ç–∏'
        }
        
        text_lower = text.lower()
        for keyword, topic in topic_keywords.items():
            if keyword in text_lower:
                book_info['topic'] = topic
                break
        
        return book_info
    
    def create_clean_prompt(self, text_chunk: str, chunk_number: int, total_chunks: int, 
                           book_info: Dict[str, str]) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text_chunk: –ß–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
            chunk_number: –ù–æ–º–µ—Ä —á–∞—Å—Ç–∏
            total_chunks: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å—Ç–µ–π
            book_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–Ω–∏–≥–µ
            
        Returns:
            –ü—Ä–æ–º–ø—Ç –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
        """
        return f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∏—Ç–∞–µ–º—ã—Ö –≤–µ—Ä—Å–∏–π –∫–Ω–∏–≥. –û–±—Ä–∞–±–æ—Ç–∞–π —á–∞—Å—Ç—å {chunk_number} –∏–∑ {total_chunks}.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–ù–ò–ì–ï:
- –ù–∞–∑–≤–∞–Ω–∏–µ: {book_info['title']}
- –ê–≤—Ç–æ—Ä: {book_info['author']}
- –¢–µ–º–∞: {book_info['topic']}

–ó–ê–î–ê–ß–ò:

1. –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï:
   - –£–±–µ—Ä–∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
   - –°–∫–ª–µ–π —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–ø—Å–∏—Ö–æ-–∞–Ω–∞–ª–∏–∑" ‚Üí "–ø—Å–∏—Ö–æ–∞–Ω–∞–ª–∏–∑")
   - –ò—Å–ø—Ä–∞–≤—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
   - –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–±–∑–∞—Ü–µ–≤

2. –§–ò–õ–¨–¢–†–ê–¶–ò–Ø (–£–î–ê–õ–ò):
   - ISBN –Ω–æ–º–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä: "ISBN 978-5-88230-251-0")
   - –ù–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü (–Ω–∞–ø—Ä–∏–º–µ—Ä: "<–Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã> <–∞–≤—Ç–æ—Ä/–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏>")
   - –ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ (–£–î–ö, –ë–ë–ö, –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞)
   - –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ–± –∞–≤—Ç–æ—Ä—Å–∫–∏—Ö –ø—Ä–∞–≤–∞—Ö (¬©, Copyright)
   - –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–µ–¥–∏—Å–ª–æ–≤–∏—è
   - –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–º–µ—Ç–∫–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

3. –£–õ–£–ß–®–ï–ù–ò–ï:
   - –ò—Å–ø—Ä–∞–≤—å –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
   - –î–æ–±–∞–≤—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
   - –ò—Å–ø—Ä–∞–≤—å —Ä–µ–≥–∏—Å—Ç—Ä –±—É–∫–≤
   - –°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ —á–∏—Ç–∞–µ–º—ã–º

4. –í–´–î–ï–õ–ï–ù–ò–ï –°–û–î–ï–†–ñ–ê–ù–ò–Ø:
   - –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –≤–∞–∂–Ω—ã–µ –∏–¥–µ–∏ –∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
   - –ü–æ–¥—á–µ—Ä–∫–Ω–∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
   - –°–æ—Ö—Ä–∞–Ω–∏ –Ω–∞—É—á–Ω—ã–π —Å—Ç–∏–ª—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å

5. –°–¢–ò–õ–¨:
   - –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç–∏–ª—å –∞–≤—Ç–æ—Ä–∞
   - –ù–µ –º–µ–Ω—è–π —Å–º—ã—Å–ª –∏ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é
   - –°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ –ø–ª–∞–≤–Ω—ã–º –¥–ª—è —á—Ç–µ–Ω–∏—è
   - –£–±–µ—Ä–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã PDF

–ò–°–•–û–î–ù–´–ô –¢–ï–ö–°–¢:
{text_chunk}

–û–ß–ò–©–ï–ù–ù–´–ô –¢–ï–ö–°–¢:"""
    
    def split_text_intelligently(self, text: str) -> List[str]:
        """
        –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π —Ç–µ–∫—Å—Ç–∞
        """
        # –°–Ω–∞—á–∞–ª–∞ —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
            if len(paragraph) > self.chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                sentences = re.split(r'(?<=[.!?])\s+', paragraph)
                temp_chunk = ""
                
                for sentence in sentences:
                    if len(temp_chunk) + len(sentence) > self.chunk_size and temp_chunk:
                        chunks.append(temp_chunk.strip())
                        temp_chunk = sentence
                    else:
                        if temp_chunk:
                            temp_chunk += " " + sentence
                        else:
                            temp_chunk = sentence
                
                if temp_chunk:
                    current_chunk = temp_chunk
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –ª–∏–º–∏—Ç
                if len(current_chunk) + len(paragraph) > self.chunk_size and current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                else:
                    if current_chunk:
                        current_chunk += "\n\n" + paragraph
                    else:
                        current_chunk = paragraph
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def process_chunk_with_retry(self, text_chunk: str, chunk_number: int, 
                               total_chunks: int, book_info: Dict[str, str], 
                               retry_count: int = 5) -> Optional[str]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
        
        Args:
            text_chunk: –ß–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
            chunk_number: –ù–æ–º–µ—Ä —á–∞—Å—Ç–∏
            total_chunks: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å—Ç–µ–π
            book_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–Ω–∏–≥–µ
            retry_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
            
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ None
        """
        prompt = self.create_clean_prompt(text_chunk, chunk_number, total_chunks, book_info)
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens
        }
        
        for attempt in range(retry_count):
            try:
                self.stats['api_calls'] += 1
                
                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=90
                )
                
                if response.status_code == 200:
                    result = response.json()
                    processed_text = result['choices'][0]['message']['content'].strip()
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤
                    if 'usage' in result:
                        self.stats['total_tokens_used'] += result['usage']['total_tokens']
                    
                    return processed_text
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ API (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {response.status_code}")
                    if response.status_code == 429:  # Rate limit
                        wait_time = 2 ** (attempt + 1)
                        print(f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time} —Å–µ–∫—É–Ω–¥...")
                        time.sleep(wait_time)
                    elif attempt < retry_count - 1:
                        time.sleep(2 ** attempt)
                        
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < retry_count - 1:
                    time.sleep(2 ** attempt)
        
        return None
    
    def process_text_file(self, input_file: str, output_file: str, 
                         book_title: str = None, book_author: str = None) -> bool:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
        
        Args:
            input_file: –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
            output_file: –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
            book_title: –ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            book_author: –ê–≤—Ç–æ—Ä –∫–Ω–∏–≥–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            True –µ—Å–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        start_time = time.time()
        
        try:
            # –ß–∏—Ç–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª
            with open(input_file, 'r', encoding='utf-8') as f:
                text = f.read()
            
            print(f"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {input_file}")
            print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(text):,} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–Ω–∏–≥–µ
            book_info = self.detect_book_info(text)
            if book_title:
                book_info['title'] = book_title
            if book_author:
                book_info['author'] = book_author
            
            print(f"üìö –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–Ω–∏–≥–µ:")
            print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {book_info['title']}")
            print(f"   –ê–≤—Ç–æ—Ä: {book_info['author']}")
            print(f"   –¢–µ–º–∞: {book_info['topic']}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏
            chunks = self.split_text_intelligently(text)
            self.stats['total_chunks'] = len(chunks)
            print(f"üî™ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
            processed_chunks = []
            
            for i, chunk in enumerate(chunks, 1):
                print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —á–∞—Å—Ç—å {i}/{len(chunks)} ({len(chunk)} —Å–∏–º–≤–æ–ª–æ–≤)...")
                
                processed_chunk = self.process_chunk_with_retry(chunk, i, len(chunks), book_info)
                
                if processed_chunk:
                    processed_chunks.append(processed_chunk)
                    self.stats['processed_chunks'] += 1
                    self.stats['total_characters'] += len(processed_chunk)
                    print(f"‚úÖ –ß–∞—Å—Ç—å {i} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–∏ {i}")
                    processed_chunks.append(chunk)  # –û—Å—Ç–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
                    self.stats['failed_chunks'] += 1
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if i < len(chunks):
                    time.sleep(1.5)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–∏
            final_text = "\n\n".join(processed_chunks)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(final_text)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Ä–µ–º–µ–Ω–∏
            self.stats['processing_time'] = time.time() - start_time
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.print_statistics()
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}")
            return False
    
    def print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        print("\n" + "="*50)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        print("="*50)
        print(f"–í—Å–µ–≥–æ —á–∞—Å—Ç–µ–π: {self.stats['total_chunks']}")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ: {self.stats['processed_chunks']}")
        print(f"–û—à–∏–±–æ–∫: {self.stats['failed_chunks']}")
        print(f"API –≤—ã–∑–æ–≤–æ–≤: {self.stats['api_calls']}")
        print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {self.stats['total_tokens_used']:,}")
        print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.stats['processing_time']:.1f} —Å–µ–∫")
        print(f"–†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {self.stats['total_characters']:,} —Å–∏–º–≤–æ–ª–æ–≤")
        
        if self.stats['total_chunks'] > 0:
            success_rate = (self.stats['processed_chunks'] / self.stats['total_chunks']) * 100
            print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {success_rate:.1f}%")


def main():
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python clean_text_processor.py input.txt -o output.txt
  python clean_text_processor.py input.txt -o output.txt --title "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏" --author "–ê–≤—Ç–æ—Ä"
  python clean_text_processor.py input.txt -o output.txt --config config.env
        """
    )
    
    parser.add_argument('input_file', help='–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª')
    parser.add_argument('-o', '--output', required=True, help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    parser.add_argument('--config', help='–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ .env')
    parser.add_argument('--title', help='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–Ω–∏–≥–∏')
    parser.add_argument('--author', help='–ê–≤—Ç–æ—Ä –∫–Ω–∏–≥–∏')
    parser.add_argument('--model', choices=['default', 'budget', 'quality'], 
                       default='default', help='–ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')
    
    args = parser.parse_args()
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        processor = CleanTextProcessor(args.config)
        
        # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞
        if args.model == 'budget':
            processor.model = processor.budget_model
        elif args.model == 'quality':
            processor.model = processor.quality_model
        
        print(f"ü§ñ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: {processor.model}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
        if not Path(args.input_file).exists():
            print(f"‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª {args.input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return 1
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
        success = processor.process_text_file(
            args.input_file, 
            args.output,
            args.title,
            args.author
        )
        
        if success:
            print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {args.output}")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ")
            return 1
        
        return 0
        
    except ValueError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –∏–ª–∏ config.env —Å –≤–∞—à–∏–º API –∫–ª—é—á–æ–º")
        return 1
    except Exception as e:
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1


if __name__ == "__main__":
    exit(main()) 