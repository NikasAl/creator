#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ (—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è).

–ù–∞—Å–ª–µ–¥—É–µ—Ç –æ—Ç BaseProcessor –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
- –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- –ì–æ—Ç–æ–≤–æ–≥–æ API –∫–ª–∏–µ–Ω—Ç–∞
- –ú–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞

–ü—Ä–∏–º–µ—Ä:
  python text_processors/correction_processor_v2.py input.txt -o corrected.txt
  python text_processors/correction_processor_v2.py input.txt -o corrected.txt --config config.env --model-preset budget
"""

import json
import time
import re
import argparse
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple

from utils.base_processor import BaseProcessor, ProcessingReport, create_arg_parser


@dataclass
class Correction:
    """–ï–¥–∏–Ω–∏—á–Ω–∞—è –ø—Ä–∞–≤–∫–∞, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å—é."""
    original: str
    replacement: str
    reason: str
    start: Optional[int] = None
    end: Optional[int] = None
    chunk_index: Optional[int] = None


class InteractiveCorrector(BaseProcessor):
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM.
    
    –§—É–Ω–∫—Ü–∏–∏:
    - –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ LLM –∑–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–∏—Å–∫ –∫–æ—Ä—Ä–µ–∫—Ü–∏–π
    - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∞–≤–æ–∫ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON
    - –ü—Ä–æ—Ö–æ–¥–∏—Ç –ø–æ –ø—Ä–∞–≤–∫–∞–º –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ: –ø—Ä–∏–º–µ–Ω–∏—Ç—å, –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏–ª–∏ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
    - –û—Å–æ–±—ã–π –∞–∫—Ü–µ–Ω—Ç –Ω–∞ –∑–∞–º–µ–Ω—É –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–∏–π —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç
    """
    
    def __init__(
        self,
        config_file: Optional[str] = None,
        model: Optional[str] = None,
        model_preset: str = 'default',
        chunk_size: int = 3000
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–∫—Ç–æ—Ä–∞.
        
        Args:
            config_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            model_preset: –ü—Ä–µ—Å–µ—Ç –º–æ–¥–µ–ª–∏ ('default', 'budget', 'quality')
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        """
        super().__init__(
            config_file=config_file,
            model=model,
            model_preset=model_preset,
            chunk_size=chunk_size
        )
        
        # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–∫—Ü–∏–π
        self.temperature = 0.2
    
    def process(self, text: str) -> str:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é.
        
        –î–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ run_interactive().
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        corrections = self._get_all_corrections(text)
        result_text = text
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–æ—Ä—Ä–µ–∫—Ü–∏–π
        for hint_pos, corr in corrections:
            if self._is_effectively_identical(corr.original, corr.replacement):
                continue
            if self._looks_like_fragment_header(corr.original):
                continue
                
            pos = result_text.find(corr.original)
            if pos != -1:
                result_text = result_text[:pos] + corr.replacement + result_text[pos + len(corr.original):]
        
        return result_text
    
    def run_interactive(
        self,
        input_path: Path,
        output_path: Path,
        dry_run: bool = False
    ) -> bool:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            input_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            output_path: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
            dry_run: –¢–æ–ª—å–∫–æ –ø—Ä–æ—Å–º–æ—Ç—Ä, –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            text = input_path.read_text(encoding='utf-8')
        except Exception as e:
            self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {e}")
            return False
        
        self._report.start_time = self._report.start_time or self._report.start_time
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.split_text(text, preset='llm_processing')
        self.logger.info(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω —Ç–µ–∫—Å—Ç: {len(text):,} —Å–∏–º–≤–æ–ª–æ–≤, —á–∞—Å—Ç–µ–π: {len(chunks)}")
        
        # –ö–∞—Ä—Ç–∞ —Å–º–µ—â–µ–Ω–∏–π —á–∞–Ω–∫–æ–≤ –≤ –æ–±—â–µ–º —Ç–µ–∫—Å—Ç–µ
        chunk_offsets = self._calculate_chunk_offsets(text, chunks)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
        all_corrections = self._collect_corrections(chunks, chunk_offsets)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
        all_corrections.sort(key=lambda x: x[0])
        
        self.logger.info(f"\n–ù–∞–π–¥–µ–Ω–æ –ø—Ä–∞–≤–æ–∫: {len(all_corrections)}")
        
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ
        current_text = text
        applied = 0
        skipped = 0
        
        for hint_pos, corr in all_corrections:
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            if self._is_effectively_identical(corr.original, corr.replacement):
                skipped += 1
                continue
            if self._looks_like_fragment_header(corr.original):
                skipped += 1
                continue
            
            # –ü–æ–∏—Å–∫ –ø–æ–∑–∏—Ü–∏–∏
            pos = self._find_in_window(current_text, corr.original, hint_pos)
            if pos is None:
                pos = current_text.find(corr.original)
                if pos == -1:
                    skipped += 1
                    continue
            
            end = pos + len(corr.original)
            ctx = self._preview_context(current_text, pos, end)
            
            # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä
            action, manual = self._prompt_user(corr, ctx)
            
            if action == 's':
                skipped += 1
                continue
            
            replacement = corr.replacement if action == 'a' else (manual if manual is not None else corr.replacement)
            
            if replacement == "":
                new_text = current_text[:pos] + current_text[end:]
            else:
                new_text = current_text[:pos] + replacement + current_text[end:]
            
            if not dry_run:
                current_text = new_text
            applied += 1
        
        self.logger.info(f"\n‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–æ: {applied}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}")
        
        if not dry_run:
            try:
                output_path.write_text(current_text, encoding='utf-8')
                self.logger.info(f"üì¶ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}")
                return False
        
        return True
    
    def _calculate_chunk_offsets(self, text: str, chunks: List[str]) -> List[int]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–º–µ—â–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –≤ –æ–±—â–µ–º —Ç–µ–∫—Å—Ç–µ."""
        chunk_offsets = []
        offset = 0
        for ch in chunks:
            pos = text.find(ch, offset)
            if pos == -1:
                pos = offset
            chunk_offsets.append(pos)
            offset = pos + len(ch)
        return chunk_offsets
    
    def _collect_corrections(
        self,
        chunks: List[str],
        chunk_offsets: List[int]
    ) -> List[Tuple[int, Correction]]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –∏–∑ –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤."""
        all_corrections = []
        
        for i, ch in enumerate(chunks, start=1):
            self.logger.info(f"üîé –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–∏ {i}/{len(chunks)}...")
            corrs = self._request_corrections(ch, i, len(chunks))
            base = chunk_offsets[i - 1]
            
            for c in corrs:
                hint = base + (c.start or 0)
                all_corrections.append((hint, c))
            
            self._report.chunks_processed += 1
            
            if i < len(chunks):
                time.sleep(0.5)
        
        return all_corrections
    
    def _request_corrections(
        self,
        chunk: str,
        idx: int,
        total: int
    ) -> List[Correction]:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —É LLM –¥–ª—è —á–∞–Ω–∫–∞."""
        prompt = self._build_correction_prompt(chunk, idx, total)
        
        try:
            response = self.call_api(prompt, max_tokens=3000)
            return self._parse_corrections(response, idx - 1)
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–π: {e}")
            return []
    
    def _build_correction_prompt(self, chunk: str, idx: int, total: int) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏."""
        return (
            "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∫–æ—Ä—Ä–µ–∫—Ç–æ—Ä. –ó–∞–¥–∞—á–∞ ‚Äî –Ω–∞–π—Ç–∏ –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏.\n"
            "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–∞–≤–∫–∞–º:\n"
            "- –ò—Å–ø—Ä–∞–≤–ª—è–π –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –≥—Ä–∞–º–º–∞—Ç–∏–∫—É.\n"
            "- –°–æ—Ö—Ä–∞–Ω—è–π —Å–º—ã—Å–ª –∞–≤—Ç–æ—Ä–∞, –Ω–µ –º–µ–Ω—è–π —Å—Ç–∏–ª—å –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.\n"
            "- –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –≤—ã—è–≤–ª—è–π –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞; –µ—Å–ª–∏ –µ—Å—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä—É—Å—Å–∫–∏–π –∞–Ω–∞–ª–æ–≥ –≤ —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ –∑–∞–º–µ–Ω—É.\n"
            "- –ù–ï –ø—Ä–µ–¥–ª–∞–≥–∞–π –ø—Ä–∞–≤–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, –º–µ—Ç–æ–∫ –∏ –Ω—É–º–µ—Ä–∞—Ç–æ—Ä–æ–≤ —Ä–∞–∑–¥–µ–ª–æ–≤, –Ω–∞–ø—Ä–∏–º–µ—Ä —Å—Ç—Ä–æ–∫ –≤–∏–¥–∞ '## –§—Ä–∞–≥–º–µ–Ω—Ç N', —Å—Ç—Ä–æ–∫ –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å '#', –∞ —Ç–∞–∫–∂–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–∞–≥–ª–∞–≤–∏–π.\n"
            "- –ù–ï –ø—Ä–µ–¥–ª–∞–≥–∞–π –ø—Ä–∞–≤–æ–∫, –µ—Å–ª–∏ –∑–∞–º–µ–Ω–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç—É.\n"
            "- –ö–∞–∂–¥–∞—è –ø—Ä–∞–≤–∫–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç–æ—á–Ω–æ–π –ø–æ–¥—Å—Ç—Ä–æ–∫–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.\n"
            "- –ü—Ä–µ–¥–ª–∞–≥–∞–π –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –∑–∞–º–µ–Ω—É, –±–µ–∑ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\n\n"
            "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî —Å—Ç—Ä–æ–≥–æ JSON —Å–æ —Å—Ö–µ–º–æ–π:\n"
            "{ 'corrections': [\n"
            "    { 'original': '—Ä–æ–≤–Ω–æ –∫–∞–∫ –≤ —Ç–µ–∫—Å—Ç–µ', 'replacement': '–Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç',\n"
            "      'reason': '–∫—Ä–∞—Ç–∫–æ', 'start': —á–∏—Å–ª–æ_–∏–ª–∏_null, 'end': —á–∏—Å–ª–æ_–∏–ª–∏_null,\n"
            "      'type': 'spelling|punctuation|grammar|foreign_word|typo|other', 'confidence': 0.0..1.0 }\n"
            "] }\n\n"
            f"–ß–∞—Å—Ç—å {idx} –∏–∑ {total}. –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∏–∂–µ –º–µ–∂–¥—É <TEXT>:</TEXT>\n"
            "<TEXT>\n" + chunk + "\n</TEXT>\n"
            "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π. –ï—Å–ª–∏ –ø—Ä–∞–≤–æ–∫ –Ω–µ—Ç, –≤–µ—Ä–Ω–∏ {\"corrections\": []}."
        )
    
    def _parse_corrections(self, response: str, chunk_index: int) -> List[Correction]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç LLM –≤ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ä–µ–∫—Ü–∏–π."""
        content = self._strip_code_fences(response)
        
        try:
            obj = json.loads(content)
        except json.JSONDecodeError:
            self.logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç–≤–µ—Ç–∞")
            return []
        
        result = []
        for c in obj.get('corrections', []):
            original = c.get('original', '') or ''
            replacement = c.get('replacement', '') or ''
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            if self._is_effectively_identical(original, replacement):
                continue
            if self._looks_like_fragment_header(original):
                continue
            
            conf = c.get('confidence')
            if isinstance(conf, (int, float)) and conf < 0.7:
                continue
            
            result.append(Correction(
                original=original,
                replacement=replacement,
                reason=c.get('reason', ''),
                start=c.get('start'),
                end=c.get('end'),
                chunk_index=chunk_index
            ))
        
        return result
    
    def _strip_code_fences(self, s: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç code fences –∏–∑ –æ—Ç–≤–µ—Ç–∞."""
        if s.startswith("```") and s.endswith("```"):
            lines = s.splitlines()
            if len(lines) >= 2:
                return "\n".join(lines[1:-1])
        return s
    
    def _is_effectively_identical(self, a: str, b: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å —Å—Ç—Ä–æ–∫ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π."""
        def norm(x: str) -> str:
            return re.sub(r"\s+", " ", x or "").strip()
        return norm(a) == norm(b)
    
    def _looks_like_fragment_header(self, s: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂–∞ –ª–∏ —Å—Ç—Ä–æ–∫–∞ –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞."""
        s = (s or '').strip()
        if re.match(r"^#+\s", s):
            return True
        if re.match(r"^##\s*–§—Ä–∞–≥–º–µ–Ω—Ç\s*\d+", s, flags=re.IGNORECASE):
            return True
        return False
    
    def _find_in_window(
        self,
        text: str,
        needle: str,
        expected_pos: int,
        window: int = 300
    ) -> Optional[int]:
        """–ò—â–µ—Ç –ø–æ–¥—Å—Ç—Ä–æ–∫—É –≤ –æ–∫–Ω–µ –≤–æ–∫—Ä—É–≥ –æ–∂–∏–¥–∞–µ–º–æ–π –ø–æ–∑–∏—Ü–∏–∏."""
        if not needle:
            return None
        
        start = max(0, expected_pos - window)
        end = min(len(text), expected_pos + window)
        segment = text[start:end]
        rel = segment.find(needle)
        
        if rel == -1:
            compact = re.sub(r"\s+", " ", segment)
            rel2 = compact.find(re.sub(r"\s+", " ", needle))
            if rel2 == -1:
                return None
            pos = text.find(needle)
            return pos if pos != -1 else None
        
        return start + rel
    
    def _preview_context(
        self,
        text: str,
        start: int,
        end: int,
        max_ctx: int = 120
    ) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–∞–≤–∫–∏."""
        left = max(0, start - max_ctx)
        right = min(len(text), end + max_ctx)
        before = text[left:start]
        target = text[start:end]
        after = text[end:right]
        return before + "<<" + target + ">>" + after
    
    def _prompt_user(
        self,
        corr: Correction,
        context: str
    ) -> Tuple[str, Optional[str]]:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –¥–µ–π—Å—Ç–≤–∏–µ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        print()
        print("-" * 80)
        print("–ü—Ä–∏—á–∏–Ω–∞:", corr.reason or "(–Ω–µ —É–∫–∞–∑–∞–Ω–æ)")
        print("–ö–æ–Ω—Ç–µ–∫—Å—Ç:")
        print(context)
        print()
        print(f"–ó–∞–º–µ–Ω–∏—Ç—å: '{corr.original}' ‚Üí '{corr.replacement}'")
        choice = input("[a] –ø—Ä–∏–º–µ–Ω–∏—Ç—å  [s] –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å  [e] –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å ‚Üí ").strip().lower() or 'a'
        
        if choice == 'e':
            manual = input("–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –∑–∞–º–µ–Ω—ã (–ø—É—Å—Ç–æ —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å): ")
            return 'e', manual
        if choice not in ('a', 's'):
            return 's', None
        return choice, None


def main():
    parser = argparse.ArgumentParser(
        description="–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç—É—Ä–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM (—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  python text_processors/correction_processor_v2.py input.txt -o corrected.txt
  python text_processors/correction_processor_v2.py input.txt -o corrected.txt --config config.env --model-preset budget
        """
    )
    
    parser.add_argument('input_file', help='–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç—É—Ä—ã')
    parser.add_argument('-o', '--output', required=True, help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏')
    parser.add_argument('--config', help='–ü—É—Ç—å –∫ .env —Ñ–∞–π–ª—É —Å –∫–ª—é—á–æ–º –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏')
    parser.add_argument('--model-preset', choices=['default', 'budget', 'quality'],
                       default='default', help='–í—ã–±–æ—Ä –ø—Ä–µ–¥–Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏')
    parser.add_argument('--chunk-size', type=int, default=3000, help='–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞')
    parser.add_argument('--dry-run', action='store_true', help='–ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–∞–π–ª, —Ç–æ–ª—å–∫–æ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä')
    parser.add_argument('--export-html', action='store_true', help='–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML –∏–∑ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞')
    parser.add_argument('--html-title', help='–ó–∞–≥–æ–ª–æ–≤–æ–∫ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã')
    
    args = parser.parse_args()
    
    try:
        corrector = InteractiveCorrector(
            config_file=args.config,
            model_preset=args.model_preset,
            chunk_size=args.chunk_size
        )
        
        in_path = Path(args.input_file)
        if not in_path.exists():
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {in_path}")
            return 1
        
        out_path = Path(args.output)
        ok = corrector.run_interactive(in_path, out_path, dry_run=args.dry_run)
        
        if not ok:
            return 1
        
        if ok and args.export_html and not args.dry_run:
            try:
                from text_processors.markdown_to_html import markdown_to_html
                html_doc = markdown_to_html(
                    out_path.read_text(encoding='utf-8'),
                    title=args.html_title or out_path.stem
                )
                html_path = out_path.with_suffix('.html')
                html_path.write_text(html_doc, encoding='utf-8')
                print(f"üåê HTML —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {html_path}")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å HTML: {e}")
        
        return 0
        
    except ValueError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        return 1
    except KeyboardInterrupt:
        print("\n‚õî –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 1
    except Exception as e:
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1


if __name__ == "__main__":
    exit(main())
