#!/usr/bin/env python3
"""
–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å–µ–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞ –≤–º–µ—Å—Ç–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    from utils.text_splitter import split_text_into_chunks

    chunks = split_text_into_chunks(text, max_chars=3000)
"""

import re
from typing import List, Optional, Callable
from dataclasses import dataclass


@dataclass
class SplitConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞."""
    max_chars: int = 3000
    preserve_paragraphs: bool = True
    split_pattern: str = r'\n\s*\n'  # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∞–±–∑–∞—Ü–µ–≤
    sentence_pattern: str = r'(?<=[.!?])\s+'  # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    respect_word_boundaries: bool = True


# –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑–≤–µ–π
PRESETS = {
    'default': SplitConfig(max_chars=3000),
    'tts_alibaba': SplitConfig(max_chars=500, preserve_paragraphs=True),
    'tts_sber': SplitConfig(max_chars=3500, preserve_paragraphs=True),
    'tts_silero': SplitConfig(max_chars=800, preserve_paragraphs=True),
    'llm_processing': SplitConfig(max_chars=10000, preserve_paragraphs=True),
    'audiobook': SplitConfig(max_chars=2500, preserve_paragraphs=True),
    'summary': SplitConfig(max_chars=6000, preserve_paragraphs=True),
}


def split_text_into_chunks(
    text: str,
    max_chars: int = 3000,
    preserve_paragraphs: bool = True,
    split_pattern: Optional[str] = None,
    sentence_pattern: Optional[str] = None,
    preset: Optional[str] = None
) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü.

    –≠—Ç–æ –µ–¥–∏–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞, –∑–∞–º–µ–Ω—è—é—â–∞—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥
    –≤ 8+ —Ñ–∞–π–ª–∞—Ö (sber_api_synth.py, silero.py, alibaba_tts.py, etc.)

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
        max_chars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ —á–∞–Ω–∫–µ
        preserve_paragraphs: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –≥—Ä–∞–Ω–∏—Ü—ã –∞–±–∑–∞—Ü–µ–≤
        split_pattern: –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∞–±–∑–∞—Ü—ã)
        sentence_pattern: –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        preset: –ò–º—è –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–∫–∏ ('tts_alibaba', 'tts_sber', 'llm_processing', etc.)
                –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è

    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞

    Examples:
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–µ—Å–µ—Ç–æ–º
        chunks = split_text_into_chunks(text, preset='tts_alibaba')

        # –ö–∞—Å—Ç–æ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        chunks = split_text_into_chunks(text, max_chars=500)

        # –î–ª—è LLM-–æ–±—Ä–∞–±–æ—Ç–∫–∏
        chunks = split_text_into_chunks(text, preset='llm_processing')
    """
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ—Å–µ—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    if preset and preset in PRESETS:
        config = PRESETS[preset]
        max_chars = config.max_chars
        preserve_paragraphs = config.preserve_paragraphs
        split_pattern = split_pattern or config.split_pattern
        sentence_pattern = sentence_pattern or config.sentence_pattern

    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = text.strip()

    if not text:
        return []

    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –º–µ–Ω—å—à–µ –ª–∏–º–∏—Ç–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if len(text) <= max_chars:
        return [text]

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    para_pattern = split_pattern or r'\n\s*\n'
    sent_pattern = sentence_pattern or r'(?<=[.!?])\s+'

    chunks = []
    current_chunk = ""

    if preserve_paragraphs:
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = re.split(para_pattern, text)

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–∞–º –ø–æ —Å–µ–±–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
            if len(paragraph) > max_chars:
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                sentences = re.split(sent_pattern, paragraph)

                for sentence in sentences:
                    sentence = sentence.strip()
                    if not sentence:
                        continue

                    # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤—Å—ë –µ—â—ë —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ - —Ä–∞–∑–±–∏–≤–∞–µ–º –∂—ë—Å—Ç–∫–æ
                    if len(sentence) > max_chars:
                        # –ñ—ë—Å—Ç–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Å —É–≤–∞–∂–µ–Ω–∏–µ–º –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤
                        words = sentence.split()
                        temp_chunk = ""

                        for word in words:
                            if len(temp_chunk) + len(word) + 1 <= max_chars:
                                temp_chunk = f"{temp_chunk} {word}".strip()
                            else:
                                if temp_chunk:
                                    chunks.append(temp_chunk)
                                temp_chunk = word

                        if temp_chunk:
                            if len(current_chunk) + len(temp_chunk) + 1 <= max_chars:
                                current_chunk = f"{current_chunk} {temp_chunk}".strip()
                            else:
                                if current_chunk:
                                    chunks.append(current_chunk)
                                current_chunk = temp_chunk
                    else:
                        # –ü—Ä–µ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
                        if len(current_chunk) + len(sentence) + 1 <= max_chars:
                            if current_chunk:
                                current_chunk = f"{current_chunk} {sentence}"
                            else:
                                current_chunk = sentence
                        else:
                            if current_chunk:
                                chunks.append(current_chunk)
                            current_chunk = sentence
            else:
                # –ê–±–∑–∞—Ü –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
                if len(current_chunk) + len(paragraph) + 2 <= max_chars:
                    if current_chunk:
                        current_chunk = f"{current_chunk}\n\n{paragraph}"
                    else:
                        current_chunk = paragraph
                else:
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = paragraph
    else:
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–±–∑–∞—Ü–µ–≤
        words = text.split()
        for word in words:
            if len(current_chunk) + len(word) + 1 <= max_chars:
                if current_chunk:
                    current_chunk = f"{current_chunk} {word}"
                else:
                    current_chunk = word
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = word

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
    if current_chunk:
        chunks.append(current_chunk)

    return chunks


def split_by_sentences(
    text: str,
    max_chars: int = 1000,
    sentence_pattern: str = r'(?<=[.!?ÿü„ÄÇ])\s+'
) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º.

    –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è TTS, –≥–¥–µ –≤–∞–∂–Ω–æ –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        max_chars: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        sentence_pattern: –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤, –∫–∞–∂–¥—ã–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–µ–ª—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    """
    sentences = re.split(sentence_pattern, text)
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        if len(current_chunk) + len(sentence) + 1 <= max_chars:
            if current_chunk:
                current_chunk = f"{current_chunk} {sentence}"
            else:
                current_chunk = sentence
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence

    if current_chunk:
        chunks.append(current_chunk)

    return chunks


def get_chunk_stats(chunks: List[str]) -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —á–∞–Ω–∫–∞–º.

    Args:
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    if not chunks:
        return {'count': 0, 'total_chars': 0, 'avg_chars': 0, 'min_chars': 0, 'max_chars': 0}

    sizes = [len(c) for c in chunks]

    return {
        'count': len(chunks),
        'total_chars': sum(sizes),
        'avg_chars': sum(sizes) // len(sizes),
        'min_chars': min(sizes),
        'max_chars': max(sizes),
    }


# === CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ===
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏")
    parser.add_argument("input_file", help="–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª —Å —Ç–µ–∫—Å—Ç–æ–º")
    parser.add_argument("--max-chars", type=int, default=3000, help="–ú–∞–∫—Å. —Å–∏–º–≤–æ–ª–æ–≤ –≤ —á–∞–Ω–∫–µ")
    parser.add_argument("--preset", choices=list(PRESETS.keys()), help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ—Å–µ—Ç")
    parser.add_argument("--stats", action="store_true", help="–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É")

    args = parser.parse_args()

    with open(args.input_file, 'r', encoding='utf-8') as f:
        text = f.read()

    if args.preset:
        chunks = split_text_into_chunks(text, preset=args.preset)
    else:
        chunks = split_text_into_chunks(text, max_chars=args.max_chars)

    print(f"–†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤:")

    for i, chunk in enumerate(chunks, 1):
        print(f"\n--- –ß–∞–Ω–∫ {i} ({len(chunk)} —Å–∏–º–≤–æ–ª–æ–≤) ---")
        print(chunk[:200] + "..." if len(chunk) > 200 else chunk)

    if args.stats:
        stats = get_chunk_stats(chunks)
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {stats['count']}")
        print(f"   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {stats['total_chars']}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä: {stats['avg_chars']}")
        print(f"   –ú–∏–Ω. —Ä–∞–∑–º–µ—Ä: {stats['min_chars']}")
        print(f"   –ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä: {stats['max_chars']}")
